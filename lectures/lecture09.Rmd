---
title: "Stats 306: Lecture 9"
subtitle: "Exploratory Data Analysis"
author: "Mark Fredrickson"
output:
  learnr::tutorial:
    progressive: true
    css: css/lecture.css
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(lubridate) # install.packages("lubridate") if you don't have this
library(Lahman) # install.packages("Lahman") if you don't have this
```






## Statistics and Data Science Workflow

![Workflow Diagram](images/r4ds-whole-game.png)

## Exploratory Data Analysis

**Exploratory Data Analysis** (EDA) is the process of learning about a particular data source and generating questions. It is an **informal** process and is **data driven**. 

* Generate questions about your data.
* Search for answers by visualizing, transforming, and modelling your data.
* Use what you learn to refine your questions and/or generate new questions.

## EDA vs. Inference

EDA is about looking at a particular data set.

**Inference** is about making informed guesses about data we **do not* observe.

Inference includes **estimating**, **testing hypotheses** or **performing prediction** with the aid of a statistical model.

EDA helps generate questions we can answer more forcefully with inference.

## Asking Questions, Maybe Answers

* EDA is about finding out what questions to ask. This is harder than it sounds.
* Quantity over quality (at least at the start)
* Our biggest questions are about **variation**: why are not all the data the same?
* We might phrase these more concretely as:
  * What type of variation occurs within my variables?
  * What type of covariation occurs between my variables? (Usually the more interesting question!)
* Getting answers is nice, but not a necessary step just yet.

## John Tukey, father of EDA

<center>

![John Tukey](images/John_Tukey.jpg)

</center>

>* The best thing about being a statistician is that you get to play in everyone's backyard.
>* The first task of the analyst of data is quantitative detective
work.
>* Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.

## Distributions and Variation

* If all our observations had the same value, we could perfectly describe our data set with a single number (or category for nominal data).
* Usually, observations differ in their values, exhibiting **variation**
* One of the primary tasks of EDA is to describe and quantify the variation
* The **distribution** of a measurement is the set of all possible values and their frequencies in the data set.
* Might look at **summaries** of the distribution to understand variation.
* Start by focusing on single measurements (**marginal distributions**), talk about **joint distributions** later
  
## Reminder: types of data

We often describe a measurement as being one of three classes:

>* Nominal/categorical: taking one of a fixed set of classes/categories
>* Ordinal: still a set of categories, but now they can be ordered
>* Quantitative (continuous): taking numeric values, possibly infinitely many

## Distributions of nominal/categorical

When observations fall into a set number of possible values, the distribution can be described with a table:

```{r}
group_by(People, bats) |> summarize(n())
```
or using this shortcut:
```{r}
count(People, bats)
```

## Proportions instead of counts

It is often useful to work on the proportions scale, as this communicates what share of the data set is contained in each level of the categorical value.

$$\frac{\text{number in group}}{\text{size of data set}}$$

```{r}
group_by(People, bats) |>
  summarize(n = n()) |>
  mutate(n / sum(n))
```

## Exercise

Calculate the proportion of each type of position (`POS`) in the `Fielding` table from Lahman.

```{r prop-field, exercise = TRUE}

```


## Displaying visually

One advantage of visual displays is that we can give (approximately) both pieces of information at the same time:

```{r}
ggplot(People, aes(x = bats)) + geom_bar()
```

## Marginal distributions for quantitative/ordinal

When we have observations that take on numeric values, or at least can be ordered, it doesn't make sense to report counts of unique values.

```{r}
summarize(People, n_distinct(weight))
```

but we can describe the **empirical cumulative distribution function**.

$$\hat F(x) = \frac{\text{number of values no larger than x}}{\text{total data set size}}$$

## Proportions and means

When we are calculating a proportion, what are we doing?

>* Finding all the units that match some condition
>* Dividing by the sample size

Suppose we wanted the proportion of players weighing less than 200 lbs:

```{r}
People_clean <- filter(People, !is.na(weight))
filter(People_clean, weight <= 200) %>% nrow() / nrow(People_clean)
```

What is `weight <= 200`?

```{r}
summarize(People_clean, class(weight <= 200))
```

R treats `TRUE` like 1 and `FALSE` like 0 so:
```{r}
summarize(People_clean, sum(weight <= 200)) / dim(People_clean)[1] 
```

But what is a sum divided by the size of the data? The mean of the condition!

```{r}
summarize(People_clean, mean(weight <= 200))
```



## Computing the ECDF 

What percentage of players have a weight no more than 200 pounds?

```{r}
summarize(People_clean, mean(weight <= 200))
```

What percentage have a weight no more than 250 pounds?
```{r}
summarize(People_clean, mean(weight <= 250))
```

## Exercise

What is the proportion of players that have made more than 100 errors? 

```{r errors-exercise, exercise = TRUE}
# group_by(Fielding, playerID) |> summarize(E = sum(E))
```

How would you write this quantity in terms of the ECDF?
## Plotting the ECDF

```{r}
ggplot(People, aes(x = weight)) + stat_ecdf() + ylab("Proportion less than x")
```

## What proportion between two values?

We could also ask questions like, what proportion between 200 and 250 pounds?

```{r}
summarize(People_clean, mean(weight > 200 & weight <= 250))
```

Notice that we could also use the ECDF to answer this:

$$\frac{\text{between 200 and 250}}{\text{total players}} = \frac{\text{less than/eq 250} - \text{less than/eq 200}}{\text{total players}} = \hat F(250) - \hat F(200)$$

```{r}
summarize(People_clean, mean(weight <= 250) - mean(weight <= 200))
```

## Showing visually
```{r}
ggplot(People_clean, aes(x = weight)) + stat_ecdf() + ylab("Proportion less than x") +
  annotate("rect", xmin = 200, xmax = 250, 
           ymin = mean(People_clean$weight <= 200), 
           ymax = mean(People_clean$weight <= 250),
  alpha = .5)
```

## More than one box

```{r}
Fhat <- function(w) { mean(People_clean$weight <= w) }

g <- ggplot(People_clean, aes(x = weight)) + stat_ecdf() + ylab("Proportion less than x")
k <- 10
b <- seq(min(People_clean$weight), max(People_clean$weight), length.out = k)
for (i in 2:k) {
  g <- g + annotate("rect", xmin = b[i - 1], xmax = b[i],
                    ymin = Fhat(b[i - 1]), ymax = Fhat(b[i]),
                    alpha = 0.5)
}
print(g)
```

## Exercise

Create an ECDF of the number of errors made for **players making fewer than 100 errors**
```{r ecdfplot-exercise, exercise = TRUE}
# group_by(Fielding, playerID) |> summarize(E = sum(E))
```

What's the largest number of errors that 75% of players have made fewer errors than that number?

Why must the point (100, 1.0) be on the line of the ECDF?


## From ECDF to histogram

```{r}
ggplot(People_clean, aes(x = weight)) + 
  geom_histogram(bins = k - 1, aes(y = stat(count / sum(count))))
```

## Next time

>* R for Data Science: 7.3 - 7.5
